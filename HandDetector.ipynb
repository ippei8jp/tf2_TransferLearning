{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HandDetector.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"history_visible":true,"mount_file_id":"13OYRjEOyrrKeGIy_aX6i3mMVe1OZ6eK2","authorship_tag":"ABX9TyOgwM9a7YJtLwKcH2nvacAJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_ksCkbvgvr5T"},"source":["参考：  https://towardsdatascience.com/train-an-object-detector-using-tensorflow-2-object-detection-api-in-2021-a4fed450d1b9"]},{"cell_type":"markdown","metadata":{"id":"J7e5tdV_vnk2"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"Z2etjP2Rou_b"},"source":["# 事前準備\n","ランタイム→ランタイムのタイプを変更を選択、ハードウェアアクセラレータを「GPU」に変更しておく。\n","Tensorflowのバージョンは2.xであることを確認しておく。\n","# GoogleDriveのマウント\n","GoogleDrive上にファイルを保存する場合は以下のセルを実行してカレントディレクトリを移動しておく。    \n","ただし、3GByte程度消費するので空き容量に注意。  \n","不要な場合はスキップする。    \n"]},{"cell_type":"code","metadata":{"id":"LxOSwlbkonaH"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L1wSuzHuo_Kk"},"source":["# ワークディレクトリの作成と移動"]},{"cell_type":"code","metadata":{"id":"rznKxR4domA1"},"source":["import sys\n","import os\n","\n","!mkdir -p hand_detect\n","%cd hand_detect\n","\n","# 現在のディレクトリ\n","CUR_DIR = os.getcwd()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eGgG286KpXYt"},"source":["# object-detection モジュールのインストール\n","## gitリポジトリのclone"]},{"cell_type":"code","metadata":{"id":"IoxRr0fxpjUQ"},"source":["%cd $CUR_DIR\n","!git clone https://github.com/tensorflow/models.git"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kZz6EdUrsZFs"},"source":["## プロトコルバッファのコンパイル"]},{"cell_type":"code","metadata":{"id":"qV-4f2jWpn3w"},"source":["%cd $CUR_DIR/models/research\n","!protoc object_detection/protos/*.proto --python_out=."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4at2q1lMssvC"},"source":["## モジュールのインストール"]},{"cell_type":"code","metadata":{"id":"OXo6bRCmqMiJ"},"source":["%cd $CUR_DIR/models/research\n","!cp object_detection/packages/tf2/setup.py . \n","!python -m pip install ."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZqXTho4itAr5"},"source":["## テスト"]},{"cell_type":"code","metadata":{"id":"z2AO1753rlr1"},"source":["%cd $CUR_DIR/models/research\n","!python object_detection/builders/model_builder_tf2_test.py"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q3qxMI7PwvAW"},"source":["# データセットのダウンロード\n","## gitリポジトリのclone"]},{"cell_type":"code","metadata":{"id":"Y1sv0LdLtHHt"},"source":["%cd $CUR_DIR\n","!git clone https://github.com/aalpatya/detect_hands.git"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yl6lDbIK3-Bq"},"source":["##  データのダウンロードとcsvファイルの生成"]},{"cell_type":"code","metadata":{"id":"a6DcC6dYw8pr"},"source":["!cp detect_hands/egohands_dataset_to_csv.py .\n","!python egohands_dataset_to_csv.py"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vmudItUyyaC0"},"source":["## tf_recordsの生成"]},{"cell_type":"code","metadata":{"id":"r7oI9Lajxkho"},"source":["%cd $CUR_DIR\n","!cp detect_hands/generate_tfrecord.py .\n","!python generate_tfrecord.py --csv_input=images/train/train_labels.csv  --output_path=train.record\n","!python generate_tfrecord.py --csv_input=images/test/test_labels.csv  --output_path=test.record"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xl6mI4uA08MZ"},"source":["# 元となるモデルのダウンロード\n","\n","元になるモデルファイルは以下を参照  \n","https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\n"]},{"cell_type":"code","metadata":{"id":"SO_X6UycyinH"},"source":["%cd $CUR_DIR\n","!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n","!tar -xzvf ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t27ugKVj656s"},"source":["## pipline.txtの修正\n","学習処理に合わせて修正する"]},{"cell_type":"code","metadata":{"id":"dp2YfZ0N6-oI"},"source":["%cd $CUR_DIR\n","\n","from object_detection.protos import pipeline_pb2\n","from google.protobuf import text_format\n","import tensorflow.compat.v1 as tf\n","\n","# CONFIGファイル名\n","CONFIG_FILE = CUR_DIR + \"/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config\"\n","ORG_FILE    = CONFIG_FILE + \",org\"\n","# CHECKPOINTファイル\n","CKPT_FILE   = CUR_DIR + \"/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0\"\n","# 学習済みモデル出力ディレクトリ\n","TRAINED_DIR = CUR_DIR + \"/output_training\"\n","\n","# CONFIGファイルのバックアップ\n","if not os.path.exists(ORG_FILE) :\n","  !cp $CONFIG_FILE $ORG_FILE\n","\n","# オリジナルファイル読み込み\n","pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n","with tf.gfile.GFile(CONFIG_FILE, \"r\") as f:\n","  proto_str = f.read()\n","  text_format.Merge(proto_str, pipeline_config)\n","\n","# パラメータ変更\n","pipeline_config.model.ssd.num_classes                                      = 1\n","\n","pipeline_config.train_config.batch_size                                    = 4\n","pipeline_config.train_config.fine_tune_checkpoint                          = CKPT_FILE\n","pipeline_config.train_config.fine_tune_checkpoint_type                     = \"detection\"\n","\n","pipeline_config.train_input_reader.label_map_path                          = CUR_DIR + \"/detect_hands/model_data/ssd_mobilenet_v2_fpn_320/label_map.pbtxt\"\n","pipeline_config.train_input_reader.tf_record_input_reader.input_path[0]    = CUR_DIR + \"/train.record\"\n","\n","pipeline_config.eval_input_reader[0].label_map_path                        = CUR_DIR + \"/detect_hands/model_data/ssd_mobilenet_v2_fpn_320/label_map.pbtxt\"\n","pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[0]  = CUR_DIR + \"/test.record\"\n","\n","# 変更後データの書き込み\n","pipeline_text = text_format.MessageToString(pipeline_config)\n","with tf.gfile.Open(CONFIG_FILE, \"wb\") as f:\n","  f.write(pipeline_text)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8qsBR_waUmwp"},"source":["# TensorBoardの起動"]},{"cell_type":"code","metadata":{"id":"HZcQeTHNUrSp"},"source":["%load_ext tensorboard\n","%tensorboard --logdir=$TRAINED_DIR/train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L5hqGiXmVRy3"},"source":["# 学習の実行\n","メインイベント  \n","ちょっと時間がかかるのでお茶でも飲んでてください(数時間のオーダー)"]},{"cell_type":"code","metadata":{"id":"cgLESxOPVXHx"},"source":["%cd $CUR_DIR/models/research/object_detection/\n","\n","!python model_main_tf2.py \\\n","--pipeline_config_path=$CONFIG_FILE \\\n","--model_dir=$TRAINED_DIR \\\n","--alsologtostderr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"prx67pKuywa4"},"source":["# モデルのエクスポート\n","生成したZIPファイルをダウンロードしてください。  "]},{"cell_type":"code","metadata":{"id":"Z98Yb4miy0dp"},"source":["%cd $CUR_DIR/models/research/object_detection\n","!python exporter_main_v2.py \\\n","--trained_checkpoint_dir=$TRAINED_DIR \\\n","--pipeline_config_path=$CONFIG_FILE \\\n","--output_directory $CUR_DIR/inference\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NcfTJS4Ysp87"},"source":["%cd $CUR_DIR\n","EXPORT_DIR = \"inference\"\n","\n","LABEL_MAP = 'detect_hands/model_data/ssd_mobilenet_v2_fpn_320/label_map.pbtxt'\n","\n","!cp $LABEL_MAP $EXPORT_DIR\n","\n","import datetime\n","zip_filename = datetime.datetime.now().strftime('hand_detect_%Y%m%d_%H%M%S.zip')\n","!zip -r $zip_filename $EXPORT_DIR"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wWhHVYk5h4Aq"},"source":["# テスト"]},{"cell_type":"code","metadata":{"id":"eu9AyJI3h88r"},"source":["# テスト用画像ファイルのダウンロード\n","!wget https://cdn.amebaowndme.com/madrid-prd/madrid-web/images/sites/483796/1357355de6edbc4c4b54d22faf0b0756_ce052e9b134a9dbb047a8e17c890832a.jpg -O a.jpg\n","!wget https://cdn.amebaowndme.com/madrid-prd/madrid-web/images/sites/483796/564b6ca69e9022aa1977f335a148a05a_2d642c807aaf8f5b972a0a406903447d.jpg -O b.jpg\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gZ75krjCiT0d"},"source":["import os\n","import sys\n","import cv2\n","\n","import numpy as np\n","import tensorflow as tf\n","\n","from PIL import Image\n","from IPython.display import display\n","\n","from object_detection.utils import ops as utils_ops\n","from object_detection.utils import label_map_util\n","from object_detection.utils import visualization_utils as vis_util\n","\n","# patch tf1 into `utils.ops`\n","utils_ops.tf = tf.compat.v1\n","\n","# Patch the location of gfile\n","tf.gfile = tf.io.gfile\n","\n","# ラベルマップのロード\n","PATH_TO_LABELS = CUR_DIR + '/detect_hands/model_data/ssd_mobilenet_v2_fpn_320/label_map.pbtxt'\n","category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n","\n","# テスト用イメージファイル\n","TEST_IMAGE_PATHS = [\n","                        \"a.jpg\", \n","                        \"b.jpg\",\n","                    ]\n","\n","# モデルのロード\n","detection_model = tf.saved_model.load(CUR_DIR + \"/inference/saved_model\")\n","\n","# Check the model's input signature, it expects a batch of 3-color images of type uint8:\n","print(detection_model.signatures['serving_default'].inputs)\n","\n","# And returns several outputs:\n","print(detection_model.signatures['serving_default'].output_dtypes)\n","print(detection_model.signatures['serving_default'].output_shapes)\n","\n","# 認識処理関数\n","def run_inference_for_single_image(model, image):\n","  image = np.asarray(image)\n","  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n","  input_tensor = tf.convert_to_tensor(image)\n","  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n","  input_tensor = input_tensor[tf.newaxis,...]\n","\n","  # Run inference\n","  model_fn = model.signatures['serving_default']\n","  output_dict = model_fn(input_tensor)\n","\n","  # All outputs are batches tensors.\n","  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n","  # We're only interested in the first num_detections.\n","  num_detections = int(output_dict.pop('num_detections'))\n","  output_dict = {key:value[0, :num_detections].numpy() \n","                 for key,value in output_dict.items()}\n","  output_dict['num_detections'] = num_detections\n","\n","  # detection_classes should be ints.\n","  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n","   \n","  # Handle models with masks:\n","  if 'detection_masks' in output_dict:\n","    # Reframe the the bbox mask to the image size.\n","    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n","              output_dict['detection_masks'], output_dict['detection_boxes'],\n","               image.shape[0], image.shape[1])      \n","    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n","                                       tf.uint8)\n","    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n","    \n","  return output_dict\n","\n","# 認識処理と表示\n","def show_inference(model, image_path):\n","  # 画像の読み込み\n","  image_np = np.array(Image.open(image_path))\n","  \n","  # 認識実行\n","  output_dict = run_inference_for_single_image(model, image_np)\n","  \n","  # Visualization of the results of a detection.\n","  vis_util.visualize_boxes_and_labels_on_image_array(\n","      image_np,\n","      output_dict['detection_boxes'],\n","      output_dict['detection_classes'],\n","      output_dict['detection_scores'],\n","      category_index,\n","      instance_masks=output_dict.get('detection_masks_reframed', None),\n","      use_normalized_coordinates=True,\n","      line_thickness=8)\n","  \n","  # 表示\n","  display(Image.fromarray(image_np))\n","  # ～～～ 単独実行するときの表示処理はこちら ～～～\n","  # new_image = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n","  # cv2.imshow(\"Detection Results\", new_image)  \n","  # cv2.waitKey(0)\n","  # cv2.destroyAllWindows()\n","\n","# 実行\n","for image_path in TEST_IMAGE_PATHS:\n","  show_inference(detection_model, image_path)\n","\n"],"execution_count":null,"outputs":[]}]}