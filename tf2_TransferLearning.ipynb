{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf2_TransferLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ksCkbvgvr5T"
      },
      "source": [
        "参考：  https://towardsdatascience.com/train-an-object-detector-using-tensorflow-2-object-detection-api-in-2021-a4fed450d1b9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUfYGOZAvmRK"
      },
      "source": [
        "# 事前準備\n",
        "ランタイム→ランタイムのタイプを変更を選択、ハードウェアアクセラレータを「GPU」に変更しておく。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7e5tdV_vnk2"
      },
      "source": [
        "# ターゲットの指定\n",
        "HAND: 手の認識  \n",
        "それ以外：Pascal VOC  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOLVwez_eAGJ"
      },
      "source": [
        "# TARGET_DATA=\"HAND\"\n",
        "TARGET_DATA=\"VOC\"\n",
        "print(TARGET_DATA)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2etjP2Rou_b"
      },
      "source": [
        "# GoogleDriveのマウント\n",
        "中断後の処理再開に必要なファイルをGoogleDrive上にファイルを保存するため、GoogleDriveをマウントする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxOSwlbkonaH"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# ベースディレクトリ\n",
        "BASE_DIR = \"/content\"\n",
        "\n",
        "if TARGET_DATA == \"HAND\" :\n",
        "  # ワークディレクトリ\n",
        "  WORK_DIR = '/content/drive/MyDrive/hand_detect'\n",
        "else :\n",
        "  # ワークディレクトリ\n",
        "  WORK_DIR = '/content/drive/MyDrive/voc_detect'\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive\n",
        "\n",
        "if os.path.exists(WORK_DIR) :\n",
        "  # 2回目以降の実行\n",
        "  FIRST_EXEC=False\n",
        "  print(\"2nd ececution\")\n",
        "else :\n",
        "  # 最初の実行\n",
        "  FIRST_EXEC=True\n",
        "  !mkdir -p $WORK_DIR\n",
        "  print(\"1st ececution\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGgG286KpXYt"
      },
      "source": [
        "# object-detection モジュールのインストール\n",
        "## gitリポジトリのclone ～ インストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoxRr0fxpjUQ"
      },
      "source": [
        "%cd $BASE_DIR\n",
        "!git clone --depth 1 https://github.com/tensorflow/models.git\n",
        "\n",
        "# プロトコルバッファのコンパイル\n",
        "%cd models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "# モジュールのインストール\n",
        "!cp object_detection/packages/tf2/setup.py . \n",
        "!python -m pip install .\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqXTho4itAr5"
      },
      "source": [
        "## テスト"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2AO1753rlr1"
      },
      "source": [
        "%cd $BASE_DIR/models/research\n",
        "!python object_detection/builders/model_builder_tf2_test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3qxMI7PwvAW"
      },
      "source": [
        "# データセットのダウンロード\n",
        "## gitリポジトリのclone→csvファイル→tf_recorfファイル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1sv0LdLtHHt"
      },
      "source": [
        "LABEL_MAP = os.path.join(WORK_DIR, \"label_map.pbtxt\")\n",
        "TRAIN_FILE = os.path.join(WORK_DIR, \"train.record\")\n",
        "TEST_FILE  = os.path.join(WORK_DIR, \"test.record\")\n",
        "\n",
        "if TARGET_DATA == \"HAND\" :\n",
        "\n",
        "  if FIRST_EXEC :\n",
        "    %cd $BASE_DIR\n",
        "    !git clone https://github.com/aalpatya/detect_hands.git\n",
        "    !python detect_hands/egohands_dataset_to_csv.py\n",
        "    !python detect_hands/generate_tfrecord.py --csv_input=images/train/train_labels.csv  --output_path=$TRAIN_FILE\n",
        "    !python detect_hands/generate_tfrecord.py --csv_input=images/test/test_labels.csv    --output_path=$TEST_FILE\n",
        "\n",
        "    # ラベルファイルをコピー\n",
        "    ! cp ./detect_hands/model_data/ssd_mobilenet_v2_fpn_320/label_map.pbtxt $LABEL_MAP\n",
        "  else :\n",
        "      print(\"2nd ececution\")\n",
        "\n",
        "else :\n",
        "  if FIRST_EXEC :\n",
        "    %cd $BASE_DIR\n",
        "    # !wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar -O - | tar xvf -\n",
        "    # ミラーサイト使用の場合はこちら\n",
        "    !wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar  -O - | tar xvf -\n",
        "    %cd $BASE_DIR/models/research/object_detection/\n",
        "    !cp ./data/pascal_label_map.pbtxt $LABEL_MAP\n",
        "    !python dataset_tools/create_pascal_tf_record.py --label_map_path $LABEL_MAP --data_dir $BASE_DIR/VOCdevkit --year VOC2007 --set train --output_path $TRAIN_FILE\n",
        "    !python dataset_tools/create_pascal_tf_record.py --label_map_path $LABEL_MAP --data_dir $BASE_DIR/VOCdevkit --year VOC2007 --set val   --output_path $TEST_FILE\n",
        "\n",
        "  else :\n",
        "      print(\"2nd ececution\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl6mI4uA08MZ"
      },
      "source": [
        "# 元となるモデルのダウンロード\n",
        "\n",
        "元になるモデルファイルは以下を参照  \n",
        "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO_X6UycyinH"
      },
      "source": [
        "# 学習済みモデル出力ディレクトリ\n",
        "TRAINED_DIR_REL = \"output_training\"\n",
        "TRAINED_DIR = os.path.join(WORK_DIR, TRAINED_DIR_REL)\n",
        "\n",
        "# CONFIGファイル名\n",
        "CONFIG_FILE     = os.path.join(WORK_DIR, \"pipeline.config\")\n",
        "\n",
        "if FIRST_EXEC :\n",
        "  %cd $WORK_DIR\n",
        "  \n",
        "  # 元となるモデルのダウンロード\n",
        "  !wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz -O - | tar xzvf -\n",
        "\n",
        "  BASE_MODEL_DIR = os.path.join(WORK_DIR, \"ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8\")\n",
        "  # 元となるモデルのCHECKPOINTファイル\n",
        "  CKPT_FILE   =  os.path.join(BASE_MODEL_DIR, \"checkpoint/ckpt-0\")\n",
        "  # 元となるモデルのconfigファイル\n",
        "  CONFIG_FILE_ORG = os.path.join(BASE_MODEL_DIR, \"pipeline.config\")\n",
        "\n",
        "  # 作業用CONFIGファイルを作成\n",
        "  !cp $CONFIG_FILE_ORG $CONFIG_FILE\n",
        "\n",
        "  if TARGET_DATA == \"HAND\" :\n",
        "    # 変更パラメータ\n",
        "    NUM_CLASSES = 1                   # クラス数\n",
        "    NUM_STEPS = 50000                 # 回数は要検討\n",
        "    BATCH_SIZE = 4                    # バッチサイズ\n",
        "  else :\n",
        "    # 変更パラメータ\n",
        "    NUM_CLASSES = 20                  # クラス数\n",
        "    NUM_STEPS = 10000                 # 回数は要検討\n",
        "    BATCH_SIZE = 64                   # バッチサイズ\n",
        "\n",
        "  from object_detection.protos import pipeline_pb2\n",
        "  from google.protobuf import text_format\n",
        "  import tensorflow.compat.v1 as tf\n",
        "\n",
        "  # CONFIGファイル読み込み\n",
        "  pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
        "  with tf.gfile.GFile(CONFIG_FILE, \"r\") as f:\n",
        "    proto_str = f.read()\n",
        "    text_format.Merge(proto_str, pipeline_config)\n",
        "\n",
        "  # パラメータ変更\n",
        "  pipeline_config.model.ssd.num_classes                                      = NUM_CLASSES\n",
        "\n",
        "  pipeline_config.train_config.batch_size                                    = BATCH_SIZE\n",
        "  pipeline_config.train_config.fine_tune_checkpoint                          = CKPT_FILE\n",
        "  pipeline_config.train_config.num_steps                                     = NUM_STEPS\n",
        "  pipeline_config.train_config.optimizer.momentum_optimizer.learning_rate.cosine_decay_learning_rate.total_steps = NUM_STEPS\n",
        "  pipeline_config.train_config.fine_tune_checkpoint_type                     = \"detection\"\n",
        "\n",
        "  pipeline_config.train_input_reader.label_map_path                          = LABEL_MAP\n",
        "  pipeline_config.train_input_reader.tf_record_input_reader.input_path[0]    = TRAIN_FILE\n",
        "\n",
        "  pipeline_config.eval_input_reader[0].label_map_path                        = LABEL_MAP\n",
        "  pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[0]  = TEST_FILE\n",
        "\n",
        "  # 変更後データの書き込み\n",
        "  pipeline_text = text_format.MessageToString(pipeline_config)\n",
        "  with tf.gfile.Open(CONFIG_FILE, \"wb\") as f:\n",
        "    f.write(pipeline_text)\n",
        "else :\n",
        "    print(\"2nd ececution\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qsBR_waUmwp"
      },
      "source": [
        "# TensorBoardの起動"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZcQeTHNUrSp"
      },
      "source": [
        "%cd $WORK_DIR\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=$TRAINED_DIR/train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5hqGiXmVRy3"
      },
      "source": [
        "# 学習の実行\n",
        "メインイベント  \n",
        "ちょっと時間がかかるのでお茶でも飲んでてください"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgLESxOPVXHx"
      },
      "source": [
        "%cd $BASE_DIR/models/research/object_detection/\n",
        "\n",
        "!python model_main_tf2.py \\\n",
        "--pipeline_config_path=$CONFIG_FILE \\\n",
        "--model_dir=$TRAINED_DIR \\\n",
        "--checkpoint_every_n=500 \\\n",
        "--alsologtostderr\n",
        "\n",
        "# 回数変更したい場合は --num_train_steps=1000な感じのオプションを追加\n",
        "# でも、learning_rate も気にしないといけないから pipeline.config を手動で書き換えた方がいいかも"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prx67pKuywa4"
      },
      "source": [
        "# モデルのエクスポート\n",
        "モデルをエクスポートしてSavedModelを作成\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z98Yb4miy0dp"
      },
      "source": [
        "%cd $BASE_DIR/models/research/object_detection\n",
        "\n",
        "if TARGET_DATA == \"HAND\" :\n",
        "  EXPORT_DIR_REL=\"hand_detect\"\n",
        "else:\n",
        "  EXPORT_DIR_REL=\"voc_detect\"\n",
        "\n",
        "EXPORT_DIR=os.path.join(WORK_DIR, EXPORT_DIR_REL)\n",
        "\n",
        "!python exporter_main_v2.py \\\n",
        "--trained_checkpoint_dir=$TRAINED_DIR \\\n",
        "--pipeline_config_path=$CONFIG_FILE \\\n",
        "--output_directory $EXPORT_DIR\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwm15ZCFj-Jf"
      },
      "source": [
        "エクスポートしたデータをアーカイブ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcfTJS4Ysp87"
      },
      "source": [
        "%cd $WORK_DIR\n",
        "# ラベルファイルもコピー\n",
        "!cp $LABEL_MAP $EXPORT_DIR_REL\n",
        "\n",
        "# ラベルファイルをテキストファイル化しておく\n",
        "# ===================================\n",
        "import sys\n",
        "import os\n",
        "import object_detection.utils.label_map_util as label_util\n",
        "\n",
        "INPUT_LABELS_FILE  = LABEL_MAP\n",
        "OUTPUT_LABELS_FILE = os.path.join(EXPORT_DIR_REL, \"label_map.txt\")\n",
        "\n",
        "category_index = label_util.create_category_index_from_labelmap(INPUT_LABELS_FILE)\n",
        "\n",
        "with open(OUTPUT_LABELS_FILE, mode='w') as f:\n",
        "    for i in range(len(category_index)+1) :\n",
        "        try:\n",
        "            name = category_index[i][\"name\"]\n",
        "        except:\n",
        "            name = str(i)\n",
        "        \n",
        "        # print(name)\n",
        "        f.write(name + '\\n')\n",
        "# ===================================\n",
        "\n",
        "import datetime\n",
        "# 現在時刻(タイムゾーン情報付加)\n",
        "now = datetime.datetime.now().astimezone(datetime.timezone(datetime.timedelta(hours=+9)))\n",
        "# ZIPファイル名を生成\n",
        "zip_filename = now.strftime(f'{EXPORT_DIR_REL}_%Y%m%d_%H%M%S.zip')\n",
        "\n",
        "!zip -r $zip_filename $EXPORT_DIR_REL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWhHVYk5h4Aq"
      },
      "source": [
        "# テスト"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu9AyJI3h88r"
      },
      "source": [
        "%cd $BASE_DIR\n",
        "# テスト用画像ファイルのダウンロード\n",
        "if TARGET_DATA == \"HAND\" :\n",
        "  !wget https://cdn.amebaowndme.com/madrid-prd/madrid-web/images/sites/483796/1357355de6edbc4c4b54d22faf0b0756_ce052e9b134a9dbb047a8e17c890832a.jpg -O a.jpg\n",
        "  !wget https://cdn.amebaowndme.com/madrid-prd/madrid-web/images/sites/483796/564b6ca69e9022aa1977f335a148a05a_2d642c807aaf8f5b972a0a406903447d.jpg -O b.jpg\n",
        "else :\n",
        "  !wget https://prtimes.jp/i/6067/298/resize/d6067-298-418042-0.jpg -O a.jpg\n",
        "  !wget https://www.kic-car.ac.jp/theme/kic_school/img/taisho/ph-society001.jpg -O b.jpg\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ75krjCiT0d"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "# patch tf1 into `utils.ops`\n",
        "utils_ops.tf = tf.compat.v1\n",
        "\n",
        "# Patch the location of gfile\n",
        "tf.gfile = tf.io.gfile\n",
        "\n",
        "# ラベルマップのロード\n",
        "PATH_TO_LABELS = LABEL_MAP\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
        "\n",
        "# テスト用イメージファイル\n",
        "TEST_IMAGE_PATHS = [\n",
        "                        \"a.jpg\", \n",
        "                        \"b.jpg\",\n",
        "                    ]\n",
        "\n",
        "# モデルのロード\n",
        "detection_model = tf.saved_model.load(os.path.join(EXPORT_DIR, \"saved_model\"))\n",
        "\n",
        "# Check the model's input signature, it expects a batch of 3-color images of type uint8:\n",
        "print(detection_model.signatures['serving_default'].inputs)\n",
        "\n",
        "# And returns several outputs:\n",
        "print(detection_model.signatures['serving_default'].output_dtypes)\n",
        "print(detection_model.signatures['serving_default'].output_shapes)\n",
        "\n",
        "# 認識処理関数\n",
        "def run_inference_for_single_image(model, image):\n",
        "  image = np.asarray(image)\n",
        "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "  input_tensor = tf.convert_to_tensor(image)\n",
        "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "  input_tensor = input_tensor[tf.newaxis,...]\n",
        "\n",
        "  # Run inference\n",
        "  model_fn = model.signatures['serving_default']\n",
        "  output_dict = model_fn(input_tensor)\n",
        "\n",
        "  # All outputs are batches tensors.\n",
        "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "  # We're only interested in the first num_detections.\n",
        "  num_detections = int(output_dict.pop('num_detections'))\n",
        "  output_dict = {key:value[0, :num_detections].numpy() \n",
        "                 for key,value in output_dict.items()}\n",
        "  output_dict['num_detections'] = num_detections\n",
        "\n",
        "  # detection_classes should be ints.\n",
        "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
        "   \n",
        "  # Handle models with masks:\n",
        "  if 'detection_masks' in output_dict:\n",
        "    # Reframe the the bbox mask to the image size.\n",
        "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
        "               image.shape[0], image.shape[1])      \n",
        "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
        "                                       tf.uint8)\n",
        "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
        "    \n",
        "  return output_dict\n",
        "\n",
        "# 認識処理と表示\n",
        "def show_inference(model, image_path):\n",
        "  # 画像の読み込み\n",
        "  image_np = np.array(Image.open(image_path))\n",
        "  \n",
        "  # 認識実行\n",
        "  output_dict = run_inference_for_single_image(model, image_np)\n",
        "  \n",
        "  # Visualization of the results of a detection.\n",
        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks_reframed', None),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=8)\n",
        "  \n",
        "  # 表示\n",
        "  display(Image.fromarray(image_np))\n",
        "  # ～～～ 単独実行するときの表示処理はこちら ～～～\n",
        "  # new_image = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
        "  # cv2.imshow(\"Detection Results\", new_image)  \n",
        "  # cv2.waitKey(0)\n",
        "  # cv2.destroyAllWindows()\n",
        "\n",
        "# 実行\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "  show_inference(detection_model, image_path)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}